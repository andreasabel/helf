Subject:
LFMTP 2011 notification for paper 2
From:
LFMTP 2011 <lfmtp2011@easychair.org>
Date:
Sat, 25 Jun 2011 00:12:24 +0100
To:
Andreas Abel <abel@informatik.uni-muenchen.de>

Dear Andreas,

We are pleased to inform you that the paper entitled 

A Lambda Term Representation Based on Linear Ordered Logic

that you submitted to LFMTP 2011 has been accepted for presentation at the workshop. Included with this message are comments prepared by its reviewers that should help you plan the presentation and to prepare a final version of the paper. We will send you instructions soon concerning the formatting and uploading process for the final version. Please note that the deadline for our receiving this version is August 1, 2011.

Please remember that we expect each accepted paper to be presented at the workshop by at least one of its authors---its inclusion in the final proceedings is, in fact, contingent on this happening. We look forward to seeing you there---there will be an interesting collection of papers at the workshop and we expect a useful and lively discussion.

Regards,
Herman and Gopalan


----------------------- REVIEW 1 ---------------------
PAPER: 2
TITLE: A Lambda Term Representation Based on Linear Ordered Logic
AUTHORS: Andreas Abel and Nicolai Kraus

OVERALL RATING: 1 (weak accept)
REVIEWER'S CONFIDENCE: 3 (high)

This paper describes a new representation for lambda terms called ordered terms that contains more precise information about variable occurrences and that consequently obviates names for bound variables in the context of reduction and that also allows usage information about substitutions generated during reduction to be maintained more accurately; the latter feature has potential for use, for instance, in more aggressive garbage collection in an implementation context. The representation and an evaluation procedure are described, the reduction procedure is shown to be correct and is compared experimentally to standard procedures. Overall, the ideas are presented clearly and they have interesting connection to linear ordered logic and explicit substitution based implementations of reduction. While these have not been probed significantly in this paper, they can be and hence a workshop presentation can stimulate useful discussion.

Some specific points for the authors to ponder about related to their work: 

(1) Lambda terms need to maintain binding information not only for guiding substitutions during reduction but also for permitting a comparison within abstraction contexts, something needed in type checking and unification. Your "nameless" representation handles the former aspect but it is far from clear that it provides a basis for the latter. Have you thought about this issue?

% I don't understand that one.


(2) Your term structure seems quite sensitive to the way terms are traversed and rewritten. For example, usual contraction on lambda terms can be done anywhere in the term. Will your representation still work if contraction is done at embedded sites in a term rather than in an outermost fashion? 

% No, it won't and there is no (reasonable) way to fix this.

Similarly, how should the representation be used if we are interested in strong head normal forms (the usual basis for comparing terms) rather than weak head normal forms? 

% isn't it done by applying whnf to a fresh variable?

A point to note is that your evaluation procedure seems to have an outermost, call-by-value strategy built into it.

% hmm.


(3) In contexts where unification is at issue, meta variables are treated in two different ways: in the "logical" way which means that reduction substitutions cannot impact them and in a "graftable" way which means that reductions substutitions have to be suspended at these variable locations. Your treatment supports the first possibility, but can it be adapted to support the second? 

% ?


(4) Graph based reduction with associated term sharing is often used in reduction. Can such an approach be used also with your representation? This relates to item (2), which notes the sensitivity to where rewriting is done. See reference [1] below for such reduction procedures based on explicit substitutions. 

(5) The experimental comparisons are a bit weak. One reason for this is that these comparisons are in the setting of a somewhat artificial task of reducing to weak head normal forms rather than in a setting where all the different operations (such as comparison, unification, etc) on lambda terms in a computation are being performed. But even in the context of reduction, I would argue that the really meaningful comparisons should be to representations/notations for lambda terms that control substitution flow carefully. Thus, a comparison with an implementation based on the work of Sinot would be more illuminating and should be tried. 

% We do perform the different operations, don't we?


On a different note, your discussion in the introduction of lambda term representations and implementations is skewed in that it seems to ignore the rather substantial work that has been done related to these topics with respect to the Teyjus implementation of Lambda Prolog. The "seminal work" of Gregoire and Leroy, for example, is reasonably useful when very large, statically known terms have to be reduced only once and then compared. However, in computations in systems like Twelf and Lambda Prolog, reductions have often to be done on terms that are dynamically created. You should at least reference (and possibly also understand carefully) work like that in [1] below that have considered the general issues more carefully and completely. 

References
==========

[1] Gopalan Nadathur. A fine-grained notation for lambda terms and its use in intensional operations. Journal of Functional and Logic Programming 1999(2): 1--62, MIT Press, March 1999.


----------------------- REVIEW 2 ---------------------
PAPER: 2
TITLE: A Lambda Term Representation Based on Linear Ordered Logic
AUTHORS: Andreas Abel and Nicolai Kraus

OVERALL RATING: 2 (accept)
REVIEWER'S CONFIDENCE: 4 (expert)

The authors consider a term representation with the intention of
optimising references to environments during evaluation by making
terms *only* reference those variables which occur free (or become
free when passing under a binder). The stated aim is to avoid space
leaks, and to improve performance during evaluation, necessary for
type-checking and type reconstruction in dependent type theory
implementations.

This is a simple, elegant little paper, eminently suitable for the
workshop, and all the more impressive for being the work of a
Bachelor's thesis (by the second author). Accept! 

The clever trick of the paper is to ensure
that at any given leaf occurrence of a bound variable, the environment
has been so copied and divided that the only remaining binding to
substitute for it is given by a singleton. This means there is only
one bound variable place holder (Bourbaki denoted it \box), and all
the occurrence machinery is handled by integer labelling of lambda and
app nodes.

Sadly for the authors, the experimental results do not seem to bear
out their wish to see improved performance, although comparison with a
conventional, fully normalising evaluator appears to have yielded a
30--50% speed-up and correspondingly smaller memory footprint. The use
of such experimental comparison is to be encouraged, while the
nature/methodology of the comparison might be examined in slightly
closer detail. 

Related work is reasonably treated; if only for the sake of the PC
Chair (Nadathur), a reference to his work in the mid-1990s on
suspensions might have yielded some deeper points of comparison. The
authors themselves note that their real (eventual?) target is in
optimising type inference/reconstruction via unification/matching, the
situation precisely considered by Nadathur and co-workers in the
context of lambda-Prolog. 


Stylistically, the paper could be improved with more comprehensive
debugging from native English speakers (colleagues in Nottingham could
help there!), 
% DONE


and curbing of the tendency to make judgments of cost
and value as on-the-fly asides, without proper quantification or
justification (esp. in the introduction). But I don't think such
nitpicks detract from the overall message. 

Worth taking more seriously is the cost of the seemingly quite
expensive operation \(\vec{v}^{w/\vec{k}}\) defined mid-page 5, and
used in the mutual definition of evaluation and application at the
bottom of the same page. On which point, it would be really nice (even
if only for the purposes of review!) to be able to refer to these
definitions by a number, or even to a table/figure. 

The clever trick of the paper (which I really liked!) means that
during evaluation the environment must be chopped up and interleaved
with new (possibly repeated) bindings in order to ensure that exactly
the one needed binding appears at exactly the right occurrence. I must
guess that the authors' disappointment wrt the observed performance
comes from the cost of all this chopping up and interleaving (and the
associated allocation cost for the run-time?). In particular, the
interaction between clauses 3 and 6 of the aforementioned definition
of evaluation (see?) appears to be a good opportunity for some partial
evaluation/worker-wrapper optimisation. Again, Nottingham is a good
place to learn how to do this! 

Detailed comments:

Title/Abstract.

The connection with ordered logic/linear logic is spelled out as
motivation, only then to be dropped. Perhaps better to say "motivated
by", or "inspired by", rather than "based on". Also, IIRC the details,
the calculus with explicit weakening, contraction and exchange
introduced by Lengrand and Kesner should be more prominently mentioned
in the introduction. It seems most closely related to what the authors
propose here.

Introduction.

I was confused by the appeal to linearity ("the context is cut" in the
abstract, later on "free variables of an application are the disjoint
union of...") when in fact contexts are not simply chopped up, but also
have terms/variables inserted. When the example of reducing an
S-combinator application is given, the non-linearity is
evident. Perhaps it would be better simply to say something like
"explicit copying" rather than "linearity"?

p.1, para.1 "Efficiency of typechecking is mostly identical with
efficiency of evaluation". The 'mostly' is horrible without being more
precise. Is type-checking otherwise linear in term size, besides
appeals to evaluation? As for the "challenge" of evaluation, well the
given proof-theoretic strength of the type theory under consideration
is what ultimately determines how bad the cost of evaluation will
be. That you consider LF means you win a bit (only Grzegorczyk-3?
still quite painful in the worst case), but systems as strong as Coq
are going to struggle however good the term representation might be,
surely?

p.1, para 2 

"These languages are involving... are prototypical and frequently
modified and extended". Do you mean "evolving" (not yet fixed) or
"involved" (complicated)?

"are prototypes" is probably better, too. 

"compilation has not... in minor evaluation tasks that dominate
higher-order unification". Quantify "minor" and "dominate" (or else:
this is an interesting area for further work! Much work has been done
to make sure that higher-order unification is not done in full
generality, but only for patterns; is this the common "minor" case
(beta-zero, in Miller's terminology) you were considering?)

"competitive to compilation" --> "competitive with compilation" 

p.2, line 1: "bears" --> "carries" 

p.2, para 1: how real is the threat of space leaks? how is this
affected by adding defintions to the calculus (you only treat lambda
terms, rather than lambda and let, as real systems use; how much does
this affect your results?)

p.2, para 2. Here lies the source of my confusion about your 'linear'
motivations. Does this help, or hinder, the reader's understanding of
later technical definitions?

p.2, last para. "consecutively on the free variables" -->
"consecutively to the free variables" 

p.3 top. The example requires us
to expect an evaluation which builds the whole explicit substitution
before trying to reduce the generated term; what happens if you
interleave beta steps with reducing the explicit substitutions?

p.3 bottom "an ordered preterm \(u\) is an ordered term if". The
sentence has definitional force, so at least mark the defined term,
here "ordered term" typographically, or even make a fully-fledged
Definition environment, please!

p.4 top The definition of the S-combinator,and a fully-applied
instance of it, in your new representation are nice, but have taken a
while to arrive. Consider moving them forward to the introduction (the
reader might then get the idea faster than thinking about linearity?)

p.4 bottom "following entry" is ambiguous; "subsequent entry" is not. 

p.5 "large applications". Such 'spine representations' (and variables
giving rise to large applications with null spine) are ubiquitous in
sequent calculus presentations (such as those of Dyckhoff/Pinto,
Pfenning/Cerevesato, ...). Consider adding a reference?

p.5 mid-bottom. How expensive are these operations (as above)?


"neither too less nor redundant information": change "less" --> "little" at a minimum
% DONE


I first saw the separation of evaluation into a weak-head step,
together with an application algebra on weak-head normal forms, in the
1991 paper by Coquand (you cite it, but not here; remedy that?)

p.6 top. The trick to your representation seems to lie in the observation about 
free variable: empty environment
bound occurrence: singleton environment

This seems an invariant which your implementation ought to be able to
make more use of (for optimisation). Or is the saving in term
representation paid for in the cost of maintaining this invariant?

p.6 mid-bottom. The translation relation you define (name it! and
label its tabular definition!) seems to need disjoint \vec{x} and
\vec{y} in the top-right inference rule. If so, remark on it, or
globally insist on freshness in the presentation of the rules.

p.6 bottom. Bad line-break after "we notice that": use an mbox, or
rephrase to avoid the notation splitting up.

On Section 5 generally: it's good to have this material, but it is not
the most interesting or important part of the paper. It's good
hygiene/adequacy/faithfulness, but you could push it to an appendix?


p.7 bottom "the following tabular" --> "the following table". Similarly page 8, middle. 
% DONE

p.9 Experimental results. I was puzzled by the quoted accuracy of
figures, and oddities such as "3000 MHz" (surely "3GHz"?) contrasted
with "7999MiB" (surely "MB", and why not "8GB"?). Why one decimal
place of accuracy for times, yet 3 decimal places of accuracy for MB
usage (if you convert to GB?).

There has been some interesting recent work on profiling basic
datatypes and their memory footprint in haskell (recent blog posting
on haskell mailing list). Consider putting your results in the context
of existing allocation overhead caused by ghc.


Also, IIRC, the language is "haskell", not "Haskell". 
% I cannot find any evidence for this statement. 


p.10 I think it would be good to spend a bit more effort on analysing
these results. you say that while "beta normal values perform
significantly worse" (yes!: about 50% worse), "the difference is
smaller than could have been expected". What did you expect, and why?
how do you account for the difference? is it really because full
evaluation is "often necessary anyway". This kind of analysis seems
like loose talk without a bit more meat to back it up. Not enough
experimental evaluation is done in theoretical computer science, but
it would be good to do it with a little more methodological discipline
and conviction.

p.10 bottom "could be seen as an optimised version of director
strings". How? (this is my ignorance showing here). Please spell this 
out (you have the space to do so).

p.11 para. 3 Claim: "not significantly in favour". 
On what statistical, or other, measure of significance? 

p.11 para. 3 Claim: "More space leaks are to be expected in
applications like..."  Substantiate the claim, especially as you seem
to want to focus on such applications in future work (and this reader
wants you to do so, as well!). 

p.11 para. 4 "a performing term representation". Do you mean a term
representation which performs competitively, or which performs at all?
Competitively against what competition? etc...


----------------------- REVIEW 3 ---------------------
PAPER: 2
TITLE: A Lambda Term Representation Based on Linear Ordered Logic
AUTHORS: Andreas Abel and Nicolai Kraus

OVERALL RATING: 2 (accept)
REVIEWER'S CONFIDENCE: 3 (high)

The paper is inconclusive but describes an interesting experiment. We should accept it for presentation at the workshop.


